#!/usr/bin/env python3
"""
åµŒå…¥æ¨¡å‹å¯¹æ¯”æµ‹è¯•
æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ä¸­æ–‡é—®é¢˜æ£€ç´¢ä¸Šçš„è¡¨ç°
"""

from src.core.question_rag_optimized import QuestionRAGOptimized, EmbeddingModel
import time


def test_model(model_name: str, model_enum: EmbeddingModel):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("\n" + "=" * 70)
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_enum.value}")
    print("=" * 70)

    start_time = time.time()

    try:
        # åˆå§‹åŒ– RAG
        rag = QuestionRAGOptimized(
            question_file='questions.yaml',
            embedding_model=model_enum.value,
            collection_name=f"test_{model_name}"
        )

        # åŠ è½½å’Œç´¢å¼•
        if not rag.load_and_index_questions():
            print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥")
            return None

        init_time = time.time() - start_time
        print(f"â±ï¸  åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}ç§’")

        # æµ‹è¯•æ£€ç´¢
        test_contexts = [
            "ç”¨æˆ·è¯´æœ€è¿‘ç¡çœ ä¸å¥½ï¼Œç»å¸¸å¤±çœ ",
            "ç”¨æˆ·æåˆ°å¾ˆå°‘è¿åŠ¨ï¼Œæ€»æ˜¯åç€",
            "ç”¨æˆ·è¡¨ç¤ºå·¥ä½œå‹åŠ›å¾ˆå¤§"
        ]

        print(f"\nğŸ” æ£€ç´¢æµ‹è¯•:")
        retrieval_times = []

        for context in test_contexts:
            start = time.time()
            question = rag.retrieve_next_question(context, exclude_asked=False)
            retrieval_time = time.time() - start
            retrieval_times.append(retrieval_time)

            if question:
                print(f"  âœ… {context[:20]}... â†’ {question.question[:30]}...")
                print(f"     è€—æ—¶: {retrieval_time:.3f}ç§’")
            else:
                print(f"  âŒ æœªæ£€ç´¢åˆ°é—®é¢˜")

        avg_retrieval_time = sum(retrieval_times) / len(retrieval_times)

        return {
            "model": model_name,
            "init_time": init_time,
            "avg_retrieval_time": avg_retrieval_time,
            "success": True
        }

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return {
            "model": model_name,
            "error": str(e),
            "success": False
        }


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘               åµŒå…¥æ¨¡å‹å¯¹æ¯”æµ‹è¯•                                         â•‘
â•‘               Embedding Models Comparison                           â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æœ¬æµ‹è¯•å°†å¯¹æ¯”ä»¥ä¸‹åµŒå…¥æ¨¡å‹åœ¨ä¸­æ–‡é—®é¢˜æ£€ç´¢ä¸Šçš„è¡¨ç°ï¼š

1. BGE-Small-ZH (æ¨è) - BAAI å‡ºå“ï¼Œè½»é‡å¿«é€Ÿ
2. BGE-Base-ZH - BAAI å‡ºå“ï¼Œæ•ˆæœæ›´å¥½ä½†è¾ƒå¤§
3. text2vec-base-chinese - ä¸­æ–‡ä¸“ç”¨
4. Paraphrase-Multilingual (å½“å‰ä½¿ç”¨) - å¤šè¯­è¨€

æ³¨æ„ï¼š
- é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ä¸€äº›æ—¶é—´
- æ¯ä¸ªæ¨¡å‹ä¼šåˆ›å»ºç‹¬ç«‹çš„å‘é‡ç´¢å¼•
- æµ‹è¯•å®Œæˆåå¯ä»¥åˆ é™¤ test_* å¼€å¤´çš„é›†åˆ
    """)

    input("\næŒ‰ Enter å¼€å§‹æµ‹è¯•...")

    models_to_test = [
        ("bge_small", EmbeddingModel.BGE_SMALL_ZH),
        ("bge_base", EmbeddingModel.BGE_BASE_ZH),
        ("text2vec", EmbeddingModel.TEXT2VEC_BASE_CHINESE),
        ("multilingual", EmbeddingModel.PARAPHRASE_MULTILINGUAL),
    ]

    results = []

    for model_name, model_enum in models_to_test:
        result = test_model(model_name, model_enum)
        if result:
            results.append(result)

        # æ¸…ç†æ˜¾å­˜
        import gc
        gc.collect()

    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")
    print("=" * 70 + "\n")

    print(f"{'æ¨¡å‹':<30} {'åˆå§‹åŒ–(ç§’)':<15} {'æ£€ç´¢é€Ÿåº¦(ç§’)':<15} {'çŠ¶æ€'}")
    print("-" * 70)

    for result in results:
        if result['success']:
            print(f"{result['model']:<30} "
                  f"{result['init_time']:<15.2f} "
                  f"{result['avg_retrieval_time']:<15.3f} "
                  f"âœ…")
        else:
            print(f"{result['model']:<30} "
                  f"{'N/A':<15} "
                  f"{'N/A':<15} "
                  f"âŒ {result.get('error', 'Unknown')[:20]}")

    print("\n" + "=" * 70)
    print("ğŸ’¡ æ¨è:")
    print("  - é€Ÿåº¦ä¼˜å…ˆ: BAAI/bge-small-zh-v1.5 (è½»é‡å¿«é€Ÿ)")
    print("  - æ•ˆæœä¼˜å…ˆ: BAAI/bge-base-zh-v1.5 (ä¸­æ–‡æ•ˆæœæœ€å¥½)")
    print("  - å¹³è¡¡é€‰æ‹©: shibing624/text2vec-base-chinese")
    print("=" * 70)

    print("\nğŸ”§ å¦‚ä½•åˆ‡æ¢æ¨¡å‹:")
    print("  ç¼–è¾‘ src/clients/interview_client_rag.py")
    print("  ä¿®æ”¹ QuestionRAG åˆå§‹åŒ–æ—¶çš„ embedding_model å‚æ•°")
    print("""
  ä¾‹å¦‚ï¼š
  from src.core.question_rag_optimized import QuestionRAGOptimized, EmbeddingModel

  rag = QuestionRAGOptimized(
      question_file='questions.yaml',
      embedding_model=EmbeddingModel.BGE_SMALL_ZH.value  # ä½¿ç”¨ BGE Small
  )
    """)


if __name__ == "__main__":
    main()
