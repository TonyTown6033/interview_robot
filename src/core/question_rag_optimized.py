"""
é—®é¢˜æ£€ç´¢å¼•æ“ - ä¼˜åŒ–ç‰ˆï¼ˆä½¿ç”¨æ›´å¥½çš„ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼‰
æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹é€‰æ‹©ï¼š
1. text2vec-base-chineseï¼ˆæ¨èï¼‰- ä¸­æ–‡ä¼˜åŒ–
2. bge-small-zh-v1.5ï¼ˆæ¨èï¼‰- BAAI å‡ºå“
3. paraphrase-multilingualï¼ˆé»˜è®¤ï¼‰- å¤šè¯­è¨€
"""

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import yaml
import os
from dataclasses import dataclass
from enum import Enum


class EmbeddingModel(Enum):
    """æ”¯æŒçš„åµŒå…¥æ¨¡å‹"""
    # æ¨èï¼šä¸­æ–‡ä¸“ç”¨æ¨¡å‹ï¼ˆæ›´é€‚åˆä¸­æ–‡è®¿è°ˆï¼‰
    TEXT2VEC_BASE_CHINESE = "shibing624/text2vec-base-chinese"  # çº¦ 400MB
    BGE_SMALL_ZH = "BAAI/bge-small-zh-v1.5"  # çº¦ 100MBï¼Œè½»é‡å¿«é€Ÿ
    BGE_BASE_ZH = "BAAI/bge-base-zh-v1.5"  # çº¦ 400MBï¼Œæ•ˆæœæ›´å¥½

    # å¤‡é€‰ï¼šå¤šè¯­è¨€æ¨¡å‹
    PARAPHRASE_MULTILINGUAL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

    # å¤‡é€‰ï¼šOpenAIï¼ˆéœ€è¦ API keyï¼‰
    OPENAI_TEXT_EMBEDDING_3_SMALL = "openai:text-embedding-3-small"
    OPENAI_TEXT_EMBEDDING_3_LARGE = "openai:text-embedding-3-large"


@dataclass
class Question:
    """é—®é¢˜æ•°æ®ç±»"""
    id: int
    question: str
    type: str
    category: Optional[str] = None
    keywords: Optional[List[str]] = None
    follow_up_hints: Optional[List[str]] = None


class QuestionRAGOptimized:
    """ä¼˜åŒ–çš„é—®é¢˜æ£€ç´¢å¼•æ“"""

    def __init__(
        self,
        question_file: str = "questions.yaml",
        collection_name: str = "interview_questions",
        embedding_model: str = EmbeddingModel.BGE_SMALL_ZH.value,  # é»˜è®¤ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
        persist_directory: str = "./chroma_db",
        use_openai: bool = False,
        openai_api_key: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„ RAG å¼•æ“

        Args:
            question_file: YAML é—®é¢˜æ–‡ä»¶è·¯å¾„
            collection_name: ChromaDB é›†åˆåç§°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
            use_openai: æ˜¯å¦ä½¿ç”¨ OpenAI embeddings
            openai_api_key: OpenAI API keyï¼ˆä½¿ç”¨ OpenAI æ—¶éœ€è¦ï¼‰
        """
        self.question_file = question_file
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.use_openai = use_openai

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if use_openai:
            print(f"ğŸ”„ ä½¿ç”¨ OpenAI Embeddings: {embedding_model}")
            self.embedding_model = None
            self.openai_client = self._init_openai(openai_api_key)
            self.embedding_model_name = embedding_model.replace("openai:", "")
        else:
            print(f"ğŸ”„ åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model}")
            self._print_model_info(embedding_model)
            self.embedding_model = SentenceTransformer(embedding_model)
            self.embedding_model_name = embedding_model
            self.openai_client = None

        # åˆå§‹åŒ– ChromaDB
        print(f"ğŸ”„ åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {persist_directory}")
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        # è·å–æˆ–åˆ›å»ºé›†åˆ
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"âœ… åŠ è½½å·²æœ‰é›†åˆ: {collection_name}")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={
                    "description": "Interview questions for RAG-based retrieval",
                    "embedding_model": self.embedding_model_name
                }
            )
            print(f"âœ… åˆ›å»ºæ–°é›†åˆ: {collection_name}")

        # åŠ è½½é—®é¢˜
        self.questions: List[Question] = []
        self.asked_question_ids: set = set()

    def _print_model_info(self, model_name: str):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        model_info = {
            "shibing624/text2vec-base-chinese": {
                "name": "text2vec-base-chinese",
                "size": "çº¦ 400MB",
                "language": "ä¸­æ–‡ä¸“ç”¨",
                "performance": "â­â­â­â­â­"
            },
            "BAAI/bge-small-zh-v1.5": {
                "name": "BGE Small Chinese",
                "size": "çº¦ 100MB",
                "language": "ä¸­æ–‡ä¸“ç”¨",
                "performance": "â­â­â­â­"
            },
            "BAAI/bge-base-zh-v1.5": {
                "name": "BGE Base Chinese",
                "size": "çº¦ 400MB",
                "language": "ä¸­æ–‡ä¸“ç”¨",
                "performance": "â­â­â­â­â­"
            },
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
                "name": "Paraphrase Multilingual",
                "size": "çº¦ 120MB",
                "language": "å¤šè¯­è¨€",
                "performance": "â­â­â­"
            }
        }

        if model_name in model_info:
            info = model_info[model_name]
            print(f"   æ¨¡å‹: {info['name']}")
            print(f"   å¤§å°: {info['size']}")
            print(f"   è¯­è¨€: {info['language']}")
            print(f"   æ€§èƒ½: {info['performance']}")

    def _init_openai(self, api_key: Optional[str]):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            key = api_key or os.getenv("OPENAI_API_KEY")
            if not key:
                raise ValueError("éœ€è¦æä¾› OPENAI_API_KEY")
            return OpenAI(api_key=key)
        except ImportError:
            raise ImportError("ä½¿ç”¨ OpenAI éœ€è¦å®‰è£…: pip install openai")

    def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        if self.use_openai:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model_name,
                input=text
            )
            return response.data[0].embedding
        else:
            return self.embedding_model.encode(
                text,
                convert_to_numpy=True
            ).tolist()

    def _get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–åµŒå…¥å‘é‡"""
        if self.use_openai:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model_name,
                input=texts
            )
            return [item.embedding for item in response.data]
        else:
            return self.embedding_model.encode(
                texts,
                show_progress_bar=True,
                convert_to_numpy=True
            ).tolist()

    def load_and_index_questions(self) -> bool:
        """ä» YAML åŠ è½½é—®é¢˜å¹¶å»ºç«‹ç´¢å¼•"""
        try:
            with open(self.question_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)

            questions_data = data.get('questions', [])
            if not questions_data:
                print("âŒ æœªæ‰¾åˆ°é—®é¢˜æ•°æ®")
                return False

            self.questions = [
                Question(
                    id=q['id'],
                    question=q['question'],
                    type=q.get('type', 'open'),
                    category=q.get('category'),
                    keywords=q.get('keywords'),
                    follow_up_hints=q.get('follow_up_hints')
                )
                for q in questions_data
            ]

            print(f"ğŸ“š åŠ è½½äº† {len(self.questions)} ä¸ªé—®é¢˜")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç´¢å¼•
            current_count = self.collection.count()
            if current_count == len(self.questions):
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸€è‡´
                metadata = self.collection.metadata
                if metadata.get('embedding_model') == self.embedding_model_name:
                    print(f"âœ… å‘é‡æ•°æ®åº“å·²åŒ…å«æ‰€æœ‰é—®é¢˜ï¼Œè·³è¿‡ç´¢å¼•")
                    return True

            # å»ºç«‹å‘é‡ç´¢å¼•
            print("ğŸ”„ æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
            self._build_index()
            print("âœ… å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆ")

            return True

        except Exception as e:
            print(f"âŒ åŠ è½½é—®é¢˜å¤±è´¥: {e}")
            return False

    def _build_index(self):
        """å°†é—®é¢˜å‘é‡åŒ–å¹¶å­˜å…¥ ChromaDB"""
        # æ¸…ç©ºæ—§æ•°æ®
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "Interview questions",
                    "embedding_model": self.embedding_model_name
                }
            )
        except Exception:
            pass

        # å‡†å¤‡æ•°æ®
        documents = []
        metadatas = []
        ids = []

        for q in self.questions:
            # æ„å»ºå¯Œæ–‡æœ¬ï¼ˆç”¨äºæ›´å¥½çš„è¯­ä¹‰ç†è§£ï¼‰
            doc_text = f"{q.question}"
            if q.category:
                doc_text += f" [ç±»åˆ«: {q.category}]"
            if q.keywords:
                doc_text += f" [å…³é”®è¯: {', '.join(q.keywords)}]"

            documents.append(doc_text)
            metadatas.append({
                "id": q.id,
                "type": q.type,
                "category": q.category or "",
                "question_text": q.question
            })
            ids.append(f"q_{q.id}")

        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        print(f"   æ­£åœ¨å‘é‡åŒ– {len(documents)} ä¸ªé—®é¢˜...")
        embeddings = self._get_embeddings_batch(documents)

        # æ‰¹é‡æ’å…¥
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

        print(f"âœ… å·²ç´¢å¼• {len(documents)} ä¸ªé—®é¢˜")

    def retrieve_next_question(
        self,
        context: str,
        n_results: int = 3,
        exclude_asked: bool = True
    ) -> Optional[Question]:
        """æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡æ£€ç´¢æœ€ç›¸å…³çš„ä¸‹ä¸€ä¸ªé—®é¢˜"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self._get_embedding(context)

            # æ£€ç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results * 2, len(self.questions))
            )

            # è§£æç»“æœ
            if not results['metadatas'] or not results['metadatas'][0]:
                return None

            # é€‰æ‹©æœ€ä½³é—®é¢˜
            for metadata in results['metadatas'][0]:
                question_id = metadata['id']

                # è·³è¿‡å·²é—®è¿‡çš„é—®é¢˜
                if exclude_asked and question_id in self.asked_question_ids:
                    continue

                # æ‰¾åˆ°å¯¹åº”çš„é—®é¢˜å¯¹è±¡
                question = next((q for q in self.questions if q.id == question_id), None)
                if question:
                    return question

            # å¦‚æœæ‰€æœ‰ç›¸å…³é—®é¢˜éƒ½é—®è¿‡äº†ï¼Œè¿”å›ä»»æ„æœªé—®è¿‡çš„é—®é¢˜
            for q in self.questions:
                if q.id not in self.asked_question_ids:
                    return q

            return None

        except Exception as e:
            print(f"âŒ æ£€ç´¢é—®é¢˜å¤±è´¥: {e}")
            return None

    def get_follow_up_questions(
        self,
        current_question: Question,
        user_answer: str,
        n_results: int = 2
    ) -> List[str]:
        """æ ¹æ®å½“å‰é—®é¢˜å’Œç”¨æˆ·å›ç­”ç”Ÿæˆè¿½é—®å»ºè®®"""
        if current_question.follow_up_hints:
            return current_question.follow_up_hints[:n_results]

        generic_followups = [
            "èƒ½è¯¦ç»†è¯´è¯´å—ï¼Ÿ",
            "è¿™ç§æƒ…å†µæŒç»­å¤šä¹…äº†ï¼Ÿ",
            "æœ‰ä»€ä¹ˆå…·ä½“çš„ä¾‹å­å—ï¼Ÿ"
        ]
        return generic_followups[:n_results]

    def mark_question_asked(self, question_id: int):
        """æ ‡è®°é—®é¢˜å·²æé—®"""
        self.asked_question_ids.add(question_id)

    def reset_asked_questions(self):
        """é‡ç½®å·²æé—®è®°å½•"""
        self.asked_question_ids.clear()

    def get_all_questions(self) -> List[Question]:
        """è·å–æ‰€æœ‰é—®é¢˜"""
        return self.questions

    def get_question_by_id(self, question_id: int) -> Optional[Question]:
        """æ ¹æ®IDè·å–é—®é¢˜"""
        return next((q for q in self.questions if q.id == question_id), None)

    def get_unanswered_count(self) -> int:
        """è·å–æœªå›ç­”é—®é¢˜æ•°é‡"""
        return len(self.questions) - len(self.asked_question_ids)


def analyze_answer_completeness(question: str, answer: str) -> Dict[str, Any]:
    """åˆ†æå›ç­”å®Œæ•´æ€§"""
    answer_length = len(answer)

    if answer_length < 10:
        return {
            'is_complete': False,
            'confidence': 0.8,
            'reason': 'å›ç­”è¿‡äºç®€çŸ­'
        }

    negative_words = ['ä¸', 'æ²¡æœ‰', 'æ²¡', 'æ— ']
    if any(word in answer for word in negative_words) and answer_length < 20:
        return {
            'is_complete': False,
            'confidence': 0.6,
            'reason': 'ç®€å•å¦å®šï¼Œå¯èƒ½éœ€è¦å±•å¼€'
        }

    return {
        'is_complete': True,
        'confidence': 0.7,
        'reason': 'å›ç­”é•¿åº¦åˆç†'
    }


# å‘åå…¼å®¹ï¼šå¯¼å‡ºä¸º QuestionRAG
QuestionRAG = QuestionRAGOptimized
